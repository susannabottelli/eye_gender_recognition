{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import sklearn\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label dataset\n",
    "labels = pd.read_csv('eye_gender_data/Training_set.csv')\n",
    "\n",
    "file_paths = [[fname, \"eye_gender_data/train/\" + fname] for fname in labels[\"filename\"]]\n",
    "\n",
    "#convert filepaths to dataframe\n",
    "images = pd.DataFrame(file_paths, columns=[\"filename\", \"filepaths\"])\n",
    "\n",
    "#create train df\n",
    "train_data = pd.merge(images, labels, how=\"inner\", on=\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_1.jpg</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_2.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_3.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_4.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_5.jpg</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                          filepaths   label\n",
       "0  Image_1.jpg  eye_gender_data/train/Image_1.jpg    male\n",
       "1  Image_2.jpg  eye_gender_data/train/Image_2.jpg  female\n",
       "2  Image_3.jpg  eye_gender_data/train/Image_3.jpg  female\n",
       "3  Image_4.jpg  eye_gender_data/train/Image_4.jpg  female\n",
       "4  Image_5.jpg  eye_gender_data/train/Image_5.jpg    male"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels to 0, 1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_data['label'] = le.fit_transform(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_3.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                          filepaths  label\n",
       "0  Image_1.jpg  eye_gender_data/train/Image_1.jpg      1\n",
       "1  Image_2.jpg  eye_gender_data/train/Image_2.jpg      0\n",
       "2  Image_3.jpg  eye_gender_data/train/Image_3.jpg      0\n",
       "3  Image_4.jpg  eye_gender_data/train/Image_4.jpg      0\n",
       "4  Image_5.jpg  eye_gender_data/train/Image_5.jpg      1"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[188, 188, 189, ..., 176, 175, 175],\n",
      "       [189, 189, 188, ..., 174, 173, 172],\n",
      "       [190, 189, 188, ..., 168, 167, 167],\n",
      "       ...,\n",
      "       [133, 137, 144, ..., 168, 167, 166],\n",
      "       [134, 138, 145, ..., 165, 164, 163],\n",
      "       [135, 139, 146, ..., 163, 162, 162]], dtype=uint8), 1], [array([[167, 169, 173, ..., 194, 195, 195],\n",
      "       [168, 170, 173, ..., 193, 194, 195],\n",
      "       [171, 171, 173, ..., 192, 193, 194],\n",
      "       ...,\n",
      "       [183, 185, 189, ..., 199, 197, 196],\n",
      "       [183, 186, 189, ..., 199, 197, 195],\n",
      "       [184, 186, 190, ..., 199, 196, 195]], dtype=uint8), 0], [array([[181, 179, 177, ..., 131, 134, 136],\n",
      "       [178, 177, 174, ..., 127, 127, 128],\n",
      "       [174, 172, 170, ..., 120, 118, 116],\n",
      "       ...,\n",
      "       [126, 128, 132, ...,  99,  96,  94],\n",
      "       [127, 130, 133, ...,  98,  95,  93],\n",
      "       [128, 131, 135, ...,  98,  94,  92]], dtype=uint8), 0]]\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "#turn all images into array and all the same size\n",
    "data = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_GRAYSCALE)\n",
    "    new_img_array = cv2.resize(img_array, (100,100))\n",
    "    data.append([new_img_array, train_data[\"label\"][i]])\n",
    "    #data_list.append([train_resized_img])\n",
    "\n",
    "#check images as numerical values\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate image and labels\n",
    "X = []\n",
    "y = []\n",
    "for image in data:\n",
    "    X.append(image[0])\n",
    "    y.append(image[1])\n",
    "\n",
    "#convert x and y to np.array\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4162, 5058]))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all images the same shape and size\n",
    "X = X.reshape(-1, 100, 100, 1)\n",
    "\n",
    "#split into train and val (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN model architecture\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(100,100,1)),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Dropout(rate=0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Dropout(rate=0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.5),\n",
    "        tf.keras.layers.Dense(150, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.3),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', #trying categorical instead of binary\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "246/246 [==============================] - 133s 536ms/step - loss: 0.7684 - accuracy: 0.5807\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 127s 516ms/step - loss: 0.5622 - accuracy: 0.7154\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 104s 421ms/step - loss: 0.5104 - accuracy: 0.7583\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 122s 494ms/step - loss: 0.4596 - accuracy: 0.7888\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 99s 401ms/step - loss: 0.4232 - accuracy: 0.8139\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 97s 393ms/step - loss: 0.3819 - accuracy: 0.8341\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 98s 398ms/step - loss: 0.3546 - accuracy: 0.8460\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 122s 495ms/step - loss: 0.3359 - accuracy: 0.8631\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 117s 475ms/step - loss: 0.3256 - accuracy: 0.8647\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 117s 476ms/step - loss: 0.3042 - accuracy: 0.8747\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 121s 492ms/step - loss: 0.2871 - accuracy: 0.8845\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 118s 481ms/step - loss: 0.2850 - accuracy: 0.8858\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 119s 483ms/step - loss: 0.2738 - accuracy: 0.8850\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 122s 496ms/step - loss: 0.2648 - accuracy: 0.8905\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 121s 491ms/step - loss: 0.2591 - accuracy: 0.8968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x164c61360>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "model.fit(X_train, y_train,\n",
    "                    batch_size=30,\n",
    "                    epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 5s 91ms/step - loss: 0.2513 - accuracy: 0.8975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25128212571144104, 0.8975054025650024]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate model\n",
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename\n",
       "0  Image_1.jpg\n",
       "1  Image_2.jpg\n",
       "2  Image_3.jpg\n",
       "3  Image_4.jpg\n",
       "4  Image_5.jpg"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the order of the image's name that has been provided\n",
    "test_image_order = pd.read_csv('eye_gender_data/Testing_set.csv')\n",
    "test_image_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get images file path\n",
    "file_paths = [[fname, 'eye_gender_data/test/' + fname] for fname in test_image_order['filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>eye_gender_data/test/Image_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>eye_gender_data/test/Image_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>eye_gender_data/test/Image_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>eye_gender_data/test/Image_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>eye_gender_data/test/Image_5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                         filepaths\n",
       "0  Image_1.jpg  eye_gender_data/test/Image_1.jpg\n",
       "1  Image_2.jpg  eye_gender_data/test/Image_2.jpg\n",
       "2  Image_3.jpg  eye_gender_data/test/Image_3.jpg\n",
       "3  Image_4.jpg  eye_gender_data/test/Image_4.jpg\n",
       "4  Image_5.jpg  eye_gender_data/test/Image_5.jpg"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert filepaths to dataframe\n",
    "test_images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
    "test_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pixel_data = []\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    img_array = cv2.imread(test_images['filepaths'][i], cv2.IMREAD_GRAYSCALE)\n",
    "    new_img_array = cv2.resize(img_array, (100,100))\n",
    "    test_pixel_data.append(new_img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn data list into array\n",
    "test_pixel_data = np.array(test_pixel_data)\n",
    "\n",
    "#make images all same shape and size\n",
    "test_pixel_data = test_pixel_data.reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9565985e-04, 9.9970430e-01],\n",
       "       [7.5970501e-01, 2.4029496e-01],\n",
       "       [2.7191434e-02, 9.7280854e-01]], dtype=float32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on test data\n",
    "pred = model.predict(test_pixel_data)\n",
    "\n",
    "#check probability values\n",
    "pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert probability values into respective classes\n",
    "prediction = []\n",
    "for value in pred:\n",
    "    prediction.append(np.argmax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-process labels\n",
    "predictions = le.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = pd.DataFrame({'filename': test_images['filename'], 'label': predictions})\n",
    "res = pd.DataFrame({'label': predictions})\n",
    "res.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
